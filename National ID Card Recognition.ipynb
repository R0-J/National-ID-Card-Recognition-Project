{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c78139-5291-4739-b3f6-79992ae606dc",
   "metadata": {},
   "source": [
    "# National ID Card Recognitionrvices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc453105-6bc8-40a3-85d1-a20f7412deef",
   "metadata": {},
   "source": [
    "The project aims to develop an intelligent system capable of accurately and efficiently recognizing national ID cards using image processing and artificial intelligence technologies. National ID cards are crucial documents used for personal identification and age verification, recognized by both government and private entities.\n",
    "\n",
    "**What is a CitizenCard?**\n",
    "\n",
    "A CitizenCard is an official UK ID and proof of age card, recognized by the Home Office and almost all UK retailers and public transport providers, including UK airlines. It facilitates individuals in proving their identity in various everyday situations.\n",
    "\n",
    "**ID Card Reader Using OpenCV and Tesseract OCR Engine**\n",
    "\n",
    "In this project, the Open Source Computer Vision Library (OpenCV) is used for image processing, offering a wide range of tools and techniques for effective image and video handling. Additionally, the Tesseract OCR engine, a powerful and free open-source tool for text recognition, is employed to convert text from images into digital format.\n",
    "\n",
    "By integrating these technologies, the project aims to build a system capable of accurately reading and analyzing national ID cards, enhancing the efficiency of identity verification processes in various practical applications, such as security checks and access to services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d121cd7-01a6-4488-968a-68d45266c619",
   "metadata": {},
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35257638-1425-4680-b14c-1817cd29ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9976e",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd111c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    gray = clahe.apply(gray)\n",
    "    \n",
    "    # Apply GaussianBlur to reduce noise and improve edge detection\n",
    "    gray = cv2.medianBlur(gray, 5)\n",
    "\n",
    "     # Apply thresholding to get a binary image\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    # Further noise reduction using median blur\n",
    "    median = cv2.medianBlur(thresh, 3)\n",
    "    \n",
    "    # Morphological transformations to improve the text area\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    dilation = cv2.dilate(median, kernel, iterations=1)\n",
    "    erosion = cv2.erode(dilation, kernel, iterations=1)\n",
    "\n",
    "    # Sharpen the image to enhance text clarity\n",
    "    sharpen_kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    sharpened = cv2.filter2D(erosion, -1, sharpen_kernel)\n",
    "    \n",
    "    # Edge detection\n",
    "    edges = cv2.Canny(erosion, 30, 150)\n",
    "\n",
    "    return image, gray, sharpened, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72c960-bae2-44aa-a6be-5afebc7060bd",
   "metadata": {},
   "source": [
    "#### Insight:\n",
    "\n",
    "- **Setting Up Tesseract Path:** The path to the Tesseract OCR executable is specified to allow Tesseract to be used for text recognition.\n",
    "\n",
    "- **Image Preprocessing Function:** The preprocess_image function takes an image path as input and processes the image to enhance text recognition.\n",
    "\n",
    "- **Loading and Grayscale Conversion:** The image is loaded using cv2.imread and then converted to a grayscale image to simplify further processing.\n",
    "\n",
    "- **Contrast Enhancement:** CLAHE is applied to improve the contrast of the grayscale image, making text regions more distinct.\n",
    "\n",
    "- **Noise Reduction:** Median blur is applied twice (before and after thresholding) to reduce noise and improve the clarity of edges and text.\n",
    "\n",
    "- **Thresholding:** Converts the image to a binary image, where text areas become white on a black background, enhancing text detection.\n",
    "\n",
    "- **Morphological Transformations:** Dilation and erosion are used to refine the text areas, making them more distinguishable.\n",
    "\n",
    "- **Sharpening:** The image is sharpened using a filter to enhance text clarity further.\n",
    "\n",
    "- **Edge Detection:** Canny edge detection is applied to identify the edges in the image, which can help in further text extraction processes.\n",
    "\n",
    "The function returns the original image and several processed versions, each highlighting different aspects crucial for effective OCR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4978091-29ef-47a4-91f0-93223f807685",
   "metadata": {},
   "source": [
    "## Find contours and extract the ID card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87fcb06-536f-4b4f-b976-5622caed204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find contours and extract the ID card\n",
    "def extract_id_card(edged, image):\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    screenCnt = None\n",
    "    \n",
    "    # Loop over contours to find the ID card\n",
    "    for c in contours:\n",
    "        # Approximate the contour\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "        # If the approximated contour has four points, assume it is the ID card\n",
    "        if len(approx) == 4:\n",
    "            screenCnt = approx\n",
    "            break\n",
    "   \n",
    "    # Ensure that we have found a contour with four points\n",
    "    if screenCnt is None:\n",
    "        raise ValueError(\"Could not find an ID card contour\")\n",
    " \n",
    "    # Transform the perspective to get a top-down view of the ID card\n",
    "    pts = screenCnt.reshape(4, 2)\n",
    "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
    "    s = pts.sum(axis=1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    diff = np.diff(pts, axis=1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    (tl, tr, br, bl) = rect\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [maxWidth - 1, 0],\n",
    "        [maxWidth - 1, maxHeight - 1],\n",
    "        [0, maxHeight - 1]], dtype=\"float32\")\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warp = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "    \n",
    "    return warp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c2bf0f-c0f8-4a69-840c-b7a8fc05b870",
   "metadata": {},
   "source": [
    "#### Insight:\n",
    "\n",
    "1- **Finding Contours:** The function starts by finding contours in the provided edge-detected image using cv2.findContours. Contours are then sorted by area in descending order to prioritize larger contours.\n",
    "\n",
    "2- **Contour Approximation:** The function loops through the sorted contours to approximate their shapes. If a contour with four points is found, it is assumed to be the ID card.\n",
    "\n",
    "3- **Contour Validation:** If no contour with four points is found, the function raises an error indicating the ID card could not be detected.\n",
    "\n",
    "4- **Perspective Transformation:** Once the ID card contour is identified, the function transforms the perspective to get a top-down view of the ID card. This involves:\n",
    "- Reshaping and reordering the contour points.\n",
    "- Calculating the maximum width and height of the transformed card.\n",
    "- Creating a destination array for the transformed image.\n",
    "- Computing the perspective transformation matrix.\n",
    "- Applying the perspective transformation to obtain a corrected view of the ID card.\n",
    "\n",
    "5- **Output:** The function returns the warped (corrected perspective) image of the ID card, which can then be used for further processing, such as OCR.\n",
    "\n",
    "This process ensures that the ID card is accurately extracted from the image, regardless of its initial orientation or perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681566bd",
   "metadata": {},
   "source": [
    "# Text Recognition with Tesseract OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text using Tesseract OCR\n",
    "def extract_text(image):\n",
    "    # Convert the image to RGB (for Tesseract)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Use Tesseract to do OCR on the  image\n",
    "    custom_config = r'--oem 3 --psm 6'\n",
    "    text = pytesseract.image_to_string(rgb_image, lang='eng', config=custom_config)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49846d50-849f-44ca-af8c-2c1d7bd65553",
   "metadata": {},
   "source": [
    "#### Insight:\n",
    "\n",
    "1- **Image Conversion:** The function first converts the input image from BGR to RGB color space using cv2.cvtColor. This step is necessary because Tesseract OCR expects images in RGB format.\n",
    "\n",
    "2- **Tesseract OCR Configuration:** The function sets custom configuration options for Tesseract OCR:\n",
    "- --oem 3: Specifies the OCR Engine Mode (OEM), where mode 3 means using both the legacy engine and the LSTM (Long Short-Term Memory) neural network engine.\n",
    "- --psm 6: Specifies the Page Segmentation Mode (PSM), where mode 6 assumes a single uniform block of text.\n",
    "\n",
    "3- **Text Extraction:** The function uses pytesseract.image_to_string to perform OCR on the RGB image with the specified configuration and language set to English ('eng').\n",
    "\n",
    "4- **Return Value:** The function returns the extracted text as a string.\n",
    "\n",
    "This function effectively converts an image to a format suitable for OCR and extracts text from it using Tesseract, allowing for automated text recognition from images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06310795",
   "metadata": {},
   "source": [
    "# Data Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00105b-5db7-4db6-9c0d-c2579437fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to structure the extracted text into a pandas DataFrame\n",
    "def structure_text(text):\n",
    "    # Split the extracted text into lines and clean up the lines\n",
    "    lines = text.split('\\n')\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    # Extract the relevant information\n",
    "    info = {\n",
    "        \"card_type\": None,\n",
    "        \"Name\": None,\n",
    "        \"DoB\": None,\n",
    "        \"Expires\": None,\n",
    "        \"Number of ID\": None,\n",
    "        \"18 on\": None,\n",
    "        \"Age range\": None\n",
    "    }\n",
    "\n",
    "    # Improved parsing logic\n",
    "    try:\n",
    "        info[\"card_type\"] = lines[0]  # The first line is the card type\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if line.lower().startswith(\"name\"):\n",
    "                info[\"Name\"] = lines[i + 1] + \" \" + lines[i + 2]\n",
    "            elif 'Dob' in line or 'DoB' in line:\n",
    "                info[\"DoB\"] = lines[i + 1].strip('is PA : . PAss’ Bee re')\n",
    "            elif line.lower().startswith(\"expires on\"):\n",
    "                info[\"Expires\"] = lines[i + 1]\n",
    "            elif '5843' in line:\n",
    "                info[\"Number of ID\"] = line.strip('Ny,. ) of ')\n",
    "            else:\n",
    "                info[\"18 on\"] = ' '.join(line.split()[:3])\n",
    "                info[\"Age range\"] = ' '.join(line.split()[3:6])\n",
    "    except IndexError as e:\n",
    "        print(f\"Error parsing lines: {e}\")\n",
    "        print(f\"Lines extracted: {lines}\")\n",
    "\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame([info])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3d7db-e967-43d5-a925-388237033d5c",
   "metadata": {},
   "source": [
    "#### Insight:\n",
    "\n",
    "The structure_text function takes text extracted by OCR and structures it into a Pandas DataFrame. It first cleans and splits the text into lines. Then, it initializes a dictionary to hold specific ID card information such as the card type, name, date of birth, expiration date, ID number, and age-related details. The function attempts to parse these details from the cleaned text lines using conditional checks and assigns them to the dictionary. If an error occurs during parsing, it prints an error message. Finally, it converts the dictionary into a Pandas DataFrame for structured representation of the extracted information. This process ensures organized and easily accessible data from the OCR output.\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187ca88-9247-4b28-9e27-9f5708f41d85",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8de31c-4eae-4edf-85c6-a430f402df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the entire pipeline\n",
    "def process_id_card(image_path):\n",
    "    # Preprocess the image\n",
    "    image, gray, sharpened, edged = preprocess_image(image_path)\n",
    "    # Extract the ID card from the image\n",
    "    id_card = extract_id_card(edged, image)\n",
    "    #Extract text from the ID card using Tesseract OCR\n",
    "    text = extract_text(id_card)\n",
    "    #Structure the text into a pandas DataFrame\n",
    "    df = structure_text(text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90e6b6",
   "metadata": {},
   "source": [
    "# Testing and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ae9de-aa07-4fb0-9fad-c9795839d6b9",
   "metadata": {},
   "source": [
    "### Test using ID cards for '16-17' for young people over 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ecb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline with a sample ID card image\n",
    "image_path = 'uk-id-card-for-16-17s.png'\n",
    "df = process_id_card(image_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b5267-5bc1-4ee8-81f1-d6dc3d0e98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original image and the processed ID card\n",
    "image, gray, sharpened, edges = preprocess_image(image_path)\n",
    "id_card = extract_id_card(edges, sharpened)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Gray Image')\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('Binary Image')\n",
    "plt.imshow(sharpened, cmap='gray')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title('Edges')\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee5832b-64c7-493b-8d73-ee8d2bf311f7",
   "metadata": {},
   "source": [
    "### Test using ID cards for 'Under 16' for children\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a56a0-e863-4a6c-821b-4867bd00e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline with a sample ID card image\n",
    "image_path = 'uk-id-card-for-under16s.jpg'\n",
    "df = process_id_card(image_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a27e34-d705-48a0-9336-976bd5450003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original image and the processed ID card\n",
    "image, gray, sharpened, edges = preprocess_image(image_path)\n",
    "id_card = extract_id_card(edges, sharpened)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Gray Image')\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('Binary Image')\n",
    "plt.imshow(sharpened, cmap='gray')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title('Edges')\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b535aa06-7d40-4ee5-96cc-1b4dc741d618",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
